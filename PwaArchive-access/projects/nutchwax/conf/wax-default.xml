<?xml version="1.0"?>

<!--Nutch(WAX) configuration.  Bulk of below are overrides for nutch-default.xml
plus on the end we add a few new properties with 'wax' prefix. This file is
read just before we write the job jar and job xml out to DFS for slaves to work
on.  It overrides nutch-default.xml.  Override the settings
below by adding overrides to hadoop-site.xml
-->
<configuration>

<property>
  <name>indexer.max.tokens</name>
  <value>100000</value>
  <description>
  The maximum number of tokens that will be indexed for a single field
  in a document. This limits the amount of memory required for
  indexing, so that collections with very large files will not crash
  the indexing process by running out of memory.

  Note that this effectively truncates large documents, excluding
  from the index tokens that occur further in the document. If you
  know your source documents are large, be sure to set this value
  high enough to accomodate the expected size. If you set it to
  Integer.MAX_VALUE, then the only limit is your memory, but you
  should anticipate an OutOfMemoryError.

  Make it ten times default.
  </description>
</property>

<property>
  <name>plugin.folders</name>
  <value>wax-plugins,plugins</value>
  <description>Directories where nutch plugins are located.  Each
  element may be a relative or absolute path.  If absolute, it is used
  as is.  If relative, it is searched for on the classpath.</description>
</property>

<property>
  <name>plugin.includes</name>
  <value>protocol-http|urlfilter-regex|parse-(text|html|rss|pdf|msword|msexcel|mspowerpoint|oo|zip|swf|rtf|default)|index-(basic|wax)|query-(basic|site|url|host|wax)|summary-lucene|scoring-opic|urlnormalizer-(pass|regex|basic)</value>
  <description>Regular expression naming plugin directory names to
  include.  Any plugin not matching this expression is excluded.
  In any case you need at least include the nutch-extensionpoints plugin. By
  default Nutch includes crawling just HTML and plain text via HTTP,
  and basic indexing and search plugins.

  See also 'parse.plugin.file'.
  </description>
</property>

<property>
  <name>parse.plugin.file</name>
  <value>wax-parse-plugins.xml</value>
  <description>The name of the file that defines the associations between
  content-types and parsers.
  </description>
</property>

<property>
  <name>io.map.index.skip</name>
  <value>7</value>
    <description>Use less RAM.

  This is now a hadoop config.
  Number of index entries to skip between each entry.
  Zero by default. Setting this to values larger than zero can
  facilitate opening large map files using less memory.

  From hadoop-default.xml.
    </description>
</property>

<property>
  <name>indexer.termIndexInterval</name>
  <value>1024</value>
  <description>Determines the fraction of terms which Lucene keeps in
  RAM when searching, to facilitate random-access.  Smaller values use
  more memory but make searches somewhat faster.  Larger values use
  less memory but make searches somewhat slower.

  Default is 128.
  </description>
</property>

<property>
  <name>searcher.summary.context</name>
  <value>20</value>
  <description>
  The number of context terms to display preceding and following
  matching terms in a hit summary.
 
  Make summaries a little longer than the default.
  </description>
</property>

<property>
  <name>searcher.summary.length</name>
  <value>80</value>
  <description>
  The total number of terms to display in a hit summary.

  Make summaries a little longer than the default.
  </description>
</property>

<property>
  <name>searcher.max.hits</name>
  <value>1000</value>
  <description>
  THIS IS NO LONGER USED. INSTEAD WE USE max.fulltext.matches.ranked

  If positive, search stops after this many hits are
  found.  Setting this to small, positive values (e.g., 1000) can make
  searches much faster.  With a sorted index, the quality of the hits
  suffers little.

  Limit to a 1000.

  If this parameter is non-default, must set below search.max.time
  parameters also else java.lang.NullPointerException in
    org.apache.nutch.searcher.LuceneQueryOptimizer$LimitedCollector
    at LuceneQueryOptimizer.java:108
  </description>
</property>

<property>
  <name>searcher.max.time.tick_count</name>
  <value>10</value>
  <description>If positive value is defined here, limit search time for
  every request to this number of elapsed ticks (see the tick_length
  property below). The total maximum time for any search request will be
  then limited to tick_count * tick_length milliseconds. When search time
  is exceeded, partial results will be returned, and the total number of
  hits will be estimated.
  </description>
</property>

<property>
  <name>searcher.max.time.tick_length</name>
  <value>1000</value>
  <description>The number of milliseconds between ticks. Larger values
  reduce the timer granularity (precision). Smaller values bring more
  overhead.
  </description>
</property>

<property>
  <name>http.content.limit</name>
  <value>10485760</value>
  <description>The length limit for downloaded content, in bytes.
  If this value is nonnegative (>=0), content longer than it will be truncated;
  otherwise, no truncation at all.

  Default for nutch is 64k.  Set the value for nutchwax to be the old
  google limit of 101k.
  </description>
</property>

<!-- The below mapred.userlog configs. override defaults which purge
anything beyond a 100k and anything over 12 hours old.  Of note, if mapred
is restarted, logs for tasks of same name are overwritten.
-->
<property>
  <name>mapred.userlog.limit.kb</name>
  <value>400</value>
  <description>The maximum size of user-logs of each task.

  We're using default split of 4 so 400 instead of 100 makes
  for files of 100k each.
  </description>
</property>

<property>
  <name>mapred.userlog.purgesplits</name>
  <value>false</value>
  <description>Should the splits be purged disregarding the user-log size limit.

  For now, don't purge logs.  Default purges.
  </description>
</property>

<property>
  <name>mapred.userlog.retain.hours</name>
  <value>168</value>
  <description>The maximum time, in hours, for which the user-logs are to be 
  				retained.


  Keep them for a week rather than for 12 hours only, the default.
  </description>
</property>

<property>
  <name>fetcher.store.content</name>
  <value>false</value>
  <description>If true, fetcher will store content.
    NutchWAX doesn't store content.  It just does content parse.  Content
    is elsewhere, in ARCs.
  </description>
</property>

<property>
<name>parser.caching.forbidden.policy</name>
    <value>all</value>
    <description>If a site (or a page) requests through its robot metatags
    that it should not be shown as cached content, apply this policy. Currently
    three keywords are recognized: "none" ignores any "noarchive" directives.
    "content" doesn't show the content, but shows summaries (snippets).
    "all" doesn't show either content or summaries.</description>
</property>

<property>
  <name>wax.index.all</name>
  <value>true</value>
    <description>If set to true, all content types are indexed.  Otherwise
    we only index text/* and application.

	All types are useful if index is going to be used by wayback or
	WERA.  Otherwise, you probably do not need to index all types
	(One way to prevent images and stylesheets appearing high
	in search results, which they are wont to do until we fix
	problems in page boosting, is to set this property to
	false).
    </description>
</property>

<property>
  <name>wax.index.timeout</name>
  <value>120</value>
    <description>Time to spend indexing an ARC.  If exceeded, indexing
    thread is destroyed.  Time units are minutes.

    An ARC of all PDFs on a machine with 4 children took more than an hour
    to process.  Make default be 2 hours.
    </description>
</property>

<property>
    <name>wax.index.redirects</name>
    <value>true</value>
    <description>If true, we index redirects (status code 30x).
    </description>
</property>

<property>
  <name>wax.parse.rate.threshold</name>
    <value>10</value>
    <description>If kilobytes per second rises above this threshold,
    we'll log the URL. Default is 10.  -1 says don't log parse rate.
    </description>
</property>

<property>
    <name>wax.pdf.size.multiplicand</name>
    <value>1</value>
    <description>Nutch is set to read http.content.limit bytes of content. PDFs
    are usually larger than text/html and truncated PDFs are usually unparseable.
    When ingesting and the resource mimetype is application/pdf, we will read
    http.content.limit * wax.pdf.size.multiplicand bytes instead of just
    http.content.limit.
    </description>
</property>

<property>
    <name>wax.digest.sha1</name>
    <value>false</value>
    <description>Nutch calculates md5 digests and uses these comparing
    page content in duplicate detection step (at least).  Set this
    property to true to have ARCReader in the import arcs step
    calculate a sha1 and write the calculated digest out to the
    crawldb (Detection of duplicate pages will fail if enabled
    until the dedup is modified to handle sha1).
    </description>
</property>

<property>
  <name>wax.host</name>
  <value>localhost:8080</value>
  <description>
  Used at search time by the nutchwax webapp.
 
  The name of the server hosting collections.
  Used by the webapp conjuring URLs that point to page renderor
  (e.g. wayback).

  URLs are conjured in this fashion:

    ${wax.host}/COLLECTION/DATE/URL

  To override the COLLECTION obtained from the search result,
  add a path to wax.host: e.g. localhost:8080/web.
  </description>
</property>



<!-- PWA parameters -->


<property>
  <name>collection.type</name>
  <value>normal</value>
  <description>
    Type of collection for indexing. This has implications mostly during the LinkDb and Index phases.
    There are three types of collections:
      normal - collection from one crawl. It will be handled as one snapshot, where a version is identified by a URL.
      multiple - collection from multiple crawls. It will be handled as multiple snapshots, where a version is identified by a URL and day.
      trec - collection from TREC (Text REtrieval Conference)
  </description>
</property>

<property>
  <name>timeout.index.servers.alive</name>
  <value>120000</value>
  <description>
    The time in milliseconds for Tomcat to check if the index servers are alive.
  </description>
</property>

<property>
  <name>ipc.verbose</name>
  <value>false</value>
  <description>
    Indicates whether the IPC protocol log the call and return information.
    There are two values: false or true. It should be true only for debuging.
  </description>
</property>

<property>
  <name>number.handlers</name>
  <value>20</value>
  <description>
    Number of threads to handle requests. 
  </description>
</property>

<property>
  <name>timeout.index.servers.response</name>
  <value>30</value>
  <description>
    Timeout in seconds for index servers to respond a query.
    -1 indicates that it should be used the "searcher.max.time.tick_count" value from nutch-default.xml.
  </description>
</property> 

<property>
  <name>max.query.terms</name>
  <value>8</value>
  <description>
    The limit of query terms and terms in phrase searched in a query.
    All the others are discarded. This is necessary just for performance reasons.
  </description>
</property>
 
<property>
  <name>max.query.extra.terms</name>
  <value>6</value>
  <description>
    The limit of advanced query terms searched in a query (e.g. not, site, date, type).
    All the others are discarded. This is necessary just for performance reasons.
  </description>
</property>

<property>
  <name>max.fulltext.matches.returned</name>
  <value>100</value>
  <description>
    Maximum number of full-text query results that index servers return (after all results are ranked and collapsed).
    This is necessary just for performance reasons and to avoid DoS.
  </description>
</property>

<property>
  <name>max.fulltext.matches.ranked</name>
  <value>10000</value>
  <description>
    Maximum number of full-text query results matched and ranked. 
    There is a tradeoff between response speed and results quality. Increasing this number can achieve a higher relevance measure (e.g. MAP), but it increases the response time.
  </description>
</property> 

<property>
  <name>timeout.indexing.document</name>
  <value>120000</value>
  <description>
    Timeout to parse and index one document. There are parsers that hang, so the running thread is killed and a new one added to the thread pool.
  </description>
</property>

<property>
  <name>database.conection</name>
  <value>//t2.tomba.fccn.pt/nutchwax</value>
  <description>
    Database connection to create virtual snapshots.
    This is only used when the collection.type = multiple.
  </description>
</property>

<property>
  <name>database.username</name>
  <value>nutchwax</value>
  <description>
    Database username to create virtual snapshots.
    This is only used when the collection.type = multiple.
  </description>
</property>

<property>
  <name>database.password</name>
  <value>xxxxx</value>
  <description>
    Database password to create virtual snapshots.
    This is only used when the collection.type = multiple.
  </description>
</property>

<property>
  <name>ranking.functions</name>
  <value>31 0.023322608715538286 45 0.59394816155096997 33 0.34503713094903127 41 1.2592823789368244</value>
  <description>
    Sets the ranking functions and their weights to compute the ranking of the matching documents. Format is functionId weight functionId weight ...
    There are 64 functions enumerated in searchTests.jsp.
  </description>
</property>


</configuration>
